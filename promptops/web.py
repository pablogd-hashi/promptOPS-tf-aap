"""
PromptOps Web UI - Streamlit Application

A visual demo of infrastructure reasoning with LLMs.
Run with: streamlit run web.py
"""

import os
import re
import json
import subprocess
import streamlit as st
from pathlib import Path
from openai import OpenAI
from context_builder import get_context_with_audit, build_full_prompt

# Paths
REPO_ROOT = Path(__file__).parent.parent
TF_DIR = REPO_ROOT / "terraform"
TFVARS_PATH = TF_DIR / "terraform.tfvars"
PROMPTS_DIR = Path(__file__).parent / "prompts"

# Page config
st.set_page_config(
    page_title="PromptOps Demo",
    page_icon="üèóÔ∏è",
    layout="wide"
)

# Debug mode - set PROMPTOPS_DEBUG_CONTEXT=true to enable
DEBUG_CONTEXT = os.getenv("PROMPTOPS_DEBUG_CONTEXT", "").lower() == "true"

# Load base system prompt (without context injection)
@st.cache_data
def load_base_prompt():
    system_file = PROMPTS_DIR / "system.txt"
    planning_file = PROMPTS_DIR / "planning.txt"

    system = system_file.read_text() if system_file.exists() else "You are an infrastructure planning assistant."
    planning = planning_file.read_text() if planning_file.exists() else ""

    return f"{system}\n\n{planning}"


# Build platform context with audit trail
@st.cache_data
def load_platform_context():
    """
    Build platform context from Terraform files.
    Returns tuple of (context_string, audit_summary, files_list)
    """
    result = get_context_with_audit(TF_DIR)
    return result.platform_context, result.summary(), result.files_read


def get_final_system_prompt():
    """Get the complete system prompt with platform context injected."""
    base_prompt = load_base_prompt()
    platform_context, _, _ = load_platform_context()
    return base_prompt.replace("{PLATFORM_CONTEXT}", platform_context)


def load_existing_tfvars():
    """Load existing terraform.tfvars as a dict"""
    if not TFVARS_PATH.exists():
        return {}

    content = TFVARS_PATH.read_text()
    result = {}

    for line in content.split('\n'):
        line = line.strip()
        if not line or line.startswith('#'):
            continue
        if '=' in line:
            key, value = line.split('=', 1)
            key = key.strip()
            value = value.strip()

            # Parse value
            if value.startswith('"') and value.endswith('"'):
                result[key] = value[1:-1]
            elif value == 'true':
                result[key] = True
            elif value == 'false':
                result[key] = False
            elif value.isdigit():
                result[key] = int(value)
            else:
                try:
                    result[key] = json.loads(value)
                except:
                    result[key] = value

    return result


def save_tfvars(vars_dict):
    """Save dict to terraform.tfvars"""
    tfvars = "# Generated by PromptOps\n\n"
    for key, value in vars_dict.items():
        if isinstance(value, str):
            tfvars += f'{key} = "{value}"\n'
        elif isinstance(value, bool):
            tfvars += f'{key} = {str(value).lower()}\n'
        else:
            tfvars += f'{key} = {json.dumps(value)}\n'

    TFVARS_PATH.parent.mkdir(parents=True, exist_ok=True)
    TFVARS_PATH.write_text(tfvars)
    return tfvars


# Initialize session state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "tfvars_content" not in st.session_state:
    st.session_state.tfvars_content = ""
if "plan_output" not in st.session_state:
    st.session_state.plan_output = ""
if "last_debug_output" not in st.session_state:
    st.session_state.last_debug_output = None

# Header
st.title("üèóÔ∏è PromptOps")
st.markdown("*Tell the AI what you need. It figures out the config. You approve and execute.*")

# LLM Provider configuration
# Set PROMPTOPS_LOCAL=true to use Ollama instead of OpenAI
USE_LOCAL = os.getenv("PROMPTOPS_LOCAL", "").lower() == "true"
LOCAL_MODEL = os.getenv("PROMPTOPS_LOCAL_MODEL", "llama3.1")
LOCAL_URL = os.getenv("PROMPTOPS_LOCAL_URL", "http://localhost:11434/v1")

if USE_LOCAL:
    client = OpenAI(base_url=LOCAL_URL, api_key="ollama")
    LLM_MODEL = LOCAL_MODEL
    st.info(f"üè† Using local model: **{LOCAL_MODEL}** via {LOCAL_URL}")
else:
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        st.error("‚ö†Ô∏è OPENAI_API_KEY not set. Run: `export OPENAI_API_KEY='sk-...'` or use `PROMPTOPS_LOCAL=true` for Ollama.")
        st.stop()
    client = OpenAI(api_key=api_key)
    LLM_MODEL = os.getenv("PROMPTOPS_MODEL", "gpt-4o")

# Layout: 2 columns
col1, col2 = st.columns([1, 1])

# LEFT COLUMN: Chat
with col1:
    st.subheader("üí¨ What do you need?")

    # Chat container
    chat_container = st.container(height=300)
    with chat_container:
        for msg in st.session_state.messages:
            with st.chat_message(msg["role"]):
                st.markdown(msg["content"])

    # Chat input
    if prompt := st.chat_input("Ask for infrastructure or changes..."):
        # Add user message
        st.session_state.messages.append({"role": "user", "content": prompt})

        # Call GPT-4
        with st.spinner("Thinking..."):
            try:
                # Build the full prompt explicitly
                base_prompt = load_base_prompt()
                platform_context, _, _ = load_platform_context()
                messages, debug_output = build_full_prompt(
                    system_prompt=base_prompt,
                    platform_context=platform_context,
                    user_messages=st.session_state.messages,
                    debug=DEBUG_CONTEXT
                )

                # Log to console if debug enabled
                if DEBUG_CONTEXT and debug_output:
                    print(debug_output)

                # Store for UI display
                st.session_state.last_debug_output = debug_output

                response = client.chat.completions.create(
                    model=LLM_MODEL,
                    messages=messages,
                    temperature=0.7,
                    max_tokens=2000,
                    timeout=60
                )

                assistant_msg = response.choices[0].message.content
                st.session_state.messages.append({"role": "assistant", "content": assistant_msg})

                # Extract JSON config if present
                json_match = re.search(r'```json\s*(\{.*?\})\s*```', assistant_msg, re.DOTALL)
                if json_match:
                    try:
                        new_vars = json.loads(json_match.group(1))

                        # Merge with existing config (so partial updates work)
                        existing_vars = load_existing_tfvars()
                        existing_vars.update(new_vars)

                        # Save merged config
                        st.session_state.tfvars_content = save_tfvars(existing_vars)

                    except json.JSONDecodeError:
                        pass

            except Exception as e:
                st.session_state.messages.append({"role": "assistant", "content": f"‚ö†Ô∏è Error calling LLM: {e}"})

        st.rerun()

    # Example prompts - demonstrating platform-bounded reasoning
    st.markdown("**Try these scenarios:**")

    col_ex1, col_ex2 = st.columns(2)
    with col_ex1:
        if st.button("üñ•Ô∏è Create a VM", use_container_width=True):
            st.session_state.messages.append({"role": "user", "content": "I need a GPU VM"})
            st.rerun()
        if st.button("üí∞ Make it cheaper", use_container_width=True):
            st.session_state.messages.append({"role": "user", "content": "Make the VM cheaper"})
            st.rerun()
        if st.button("üîì Enable Streamlit", use_container_width=True):
            st.session_state.messages.append({"role": "user", "content": "Enable access to the Streamlit app"})
            st.rerun()
    with col_ex2:
        if st.button("‚ùå Use V100 (invalid)", use_container_width=True):
            st.session_state.messages.append({"role": "user", "content": "Use a V100 GPU"})
            st.rerun()
        if st.button("‚ùå Open port 9000 (invalid)", use_container_width=True):
            st.session_state.messages.append({"role": "user", "content": "Open port 9000"})
            st.rerun()
        if st.button("üîí Enable encryption", use_container_width=True):
            st.session_state.messages.append({"role": "user", "content": "Enable disk encryption"})
            st.rerun()

# RIGHT COLUMN: Config & Plan
with col2:
    st.subheader("üìÑ Configuration")

    # Load current tfvars if exists
    if TFVARS_PATH.exists() and not st.session_state.tfvars_content:
        st.session_state.tfvars_content = TFVARS_PATH.read_text()

    # Show tfvars
    tfvars_display = st.session_state.tfvars_content or "# No configuration yet\n# Chat to generate one"
    st.code(tfvars_display, language="hcl")

# Full width: Plan output and actions
st.divider()

col_plan, col_actions = st.columns([3, 1])

with col_actions:
    st.subheader("‚ö° Actions")

    # Run Plan button
    if st.button("üîç Run Plan", use_container_width=True):
        with st.spinner("Running terraform plan..."):
            try:
                result = subprocess.run(
                    ["terraform", "plan", "-no-color"],
                    cwd=TF_DIR,
                    capture_output=True,
                    text=True,
                    timeout=120
                )
                st.session_state.plan_output = result.stdout + result.stderr
            except subprocess.TimeoutExpired:
                st.session_state.plan_output = "Error: Plan timed out"
            except Exception as e:
                st.session_state.plan_output = f"Error: {e}"
        st.rerun()

    # Apply button
    if st.button("üöÄ Apply", use_container_width=True, type="primary"):
        st.warning("This creates REAL infrastructure on GCP!")
        st.session_state.show_apply_confirm = True

    if st.session_state.get("show_apply_confirm"):
        if st.button("‚úÖ Yes, apply changes", use_container_width=True):
            with st.spinner("Applying..."):
                try:
                    result = subprocess.run(
                        ["terraform", "apply", "-auto-approve", "-no-color"],
                        cwd=TF_DIR,
                        capture_output=True,
                        text=True,
                        timeout=600
                    )
                    st.session_state.plan_output = result.stdout + result.stderr
                    st.session_state.show_apply_confirm = False
                except Exception as e:
                    st.session_state.plan_output = f"Error: {e}"
            st.rerun()
        if st.button("‚ùå Cancel", use_container_width=True):
            st.session_state.show_apply_confirm = False
            st.rerun()

    # Destroy button
    if st.button("üí• Destroy", use_container_width=True):
        st.session_state.show_destroy_confirm = True

    if st.session_state.get("show_destroy_confirm"):
        st.error("This will DELETE all infrastructure!")
        if st.button("‚úÖ Yes, destroy", use_container_width=True):
            with st.spinner("Destroying..."):
                try:
                    result = subprocess.run(
                        ["terraform", "destroy", "-auto-approve", "-no-color"],
                        cwd=TF_DIR,
                        capture_output=True,
                        text=True,
                        timeout=300
                    )
                    st.session_state.plan_output = result.stdout + result.stderr
                    st.session_state.show_destroy_confirm = False
                except Exception as e:
                    st.session_state.plan_output = f"Error: {e}"
            st.rerun()
        if st.button("‚ùå Cancel", use_container_width=True, key="cancel_destroy"):
            st.session_state.show_destroy_confirm = False
            st.rerun()

    st.divider()

    # Clear chat
    if st.button("üóëÔ∏è Clear Chat", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

with col_plan:
    st.subheader("üìã Terraform Output")

    plan_display = st.session_state.plan_output or "# No output yet\n# Click 'Run Plan' to see what will change"
    st.code(plan_display, language="bash", line_numbers=False)

# Demo App Status
st.divider()

# Check if we have outputs and show app status
try:
    app_status_result = subprocess.run(
        ["terraform", "output", "-raw", "app_status"],
        cwd=TF_DIR,
        capture_output=True,
        text=True,
        timeout=5
    )

    app_accessible_result = subprocess.run(
        ["terraform", "output", "-raw", "app_accessible"],
        cwd=TF_DIR,
        capture_output=True,
        text=True,
        timeout=5
    )

    if app_status_result.returncode == 0:
        app_status = app_status_result.stdout.strip()
        is_accessible = app_accessible_result.stdout.strip() == "true"

        st.subheader("üéØ Demo App Status")

        if is_accessible:
            st.success(f"**{app_status}**")
            app_url_result = subprocess.run(
                ["terraform", "output", "-raw", "app_url"],
                cwd=TF_DIR,
                capture_output=True,
                text=True,
                timeout=5
            )
            if app_url_result.returncode == 0:
                st.markdown(f"### [Open Demo App]({app_url_result.stdout.strip()})")
        else:
            st.warning(f"**{app_status}**")
            st.info("üí° Say: *'Open access to the app'* to enable the firewall")
except:
    pass  # No outputs yet, that's fine

# Footer
st.divider()
st.caption("PromptOps: The AI reasons about infrastructure. Terraform and Ansible execute it. You approve everything.")

# LLM Context Debug Panel (only shown when PROMPTOPS_DEBUG_CONTEXT=true)
if DEBUG_CONTEXT:
    st.divider()
    with st.expander("üîç LLM Context (Debug Mode)", expanded=False):
        st.markdown("**Environment Variable:** `PROMPTOPS_DEBUG_CONTEXT=true`")
        st.markdown("This panel shows exactly what is sent to the LLM.")

        # Show files read
        st.markdown("### Files Read")
        _, audit_summary, files_read = load_platform_context()
        for f in files_read:
            status = "‚úÖ" if f.exists else "‚ùå"
            st.text(f"{status} {f.path} ({f.bytes_read} bytes, {f.variables_extracted} vars)")

        # Show platform context
        st.markdown("### Platform Context (injected into prompt)")
        platform_context, _, _ = load_platform_context()
        st.code(platform_context, language="markdown")

        # Show last full prompt sent
        if st.session_state.last_debug_output:
            st.markdown("### Last Prompt Sent to LLM")
            st.code(st.session_state.last_debug_output, language="text")
